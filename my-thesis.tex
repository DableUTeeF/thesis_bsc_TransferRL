\documentclass[12pt,a4paper]{article}
\usepackage{makeidx}
\makeindex

\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{tabto}
\usepackage{xltxtra}
\usepackage{polyglossia}
\usepackage[top=25mm, bottom=20mm, left=25mm, right=20mm]{geometry}
\XeTeXlinebreaklocale "th"
\XeTeXlinebreakskip = 0pt plus 1pt
\defaultfontfeatures{Scale=1.23}
\title{Transferable Reinforcement Learning for Board Games}
\author{Natthaphon Hongcharoen}
%% ใช้ร่วมกับ polyglossia เพื่อให้ได้วันที่ภาษาไทย
\setdefaultlanguage{english}
\newfontfamily{\thaifont}[Script=Thai]{TH SarabunPSK}

\begin{document}
\fontspec[
ItalicFont={TH SarabunPSK Italic},
BoldFont={TH SarabunPSK Bold},
BoldItalicFont={TH SarabunPSK Bold Italic},
]{TH SarabunPSK}

\begin{titlepage}
	\centering
	\includegraphics[width=0.15\textwidth]{logo.png}\par\vspace{1cm}
	{\scshape\Huge Transferable Reinforcement Learning for \par Board Games\par}
	\vfill
	{\Large\itshape Natthaphon Hongcharoen\par}
	\vfill
	{\Large  \par}
	{\large Thesis presented for Faculty of Engineering and Technology \par\vspace{0.1cm}}
	{\large Panyapiwat Insitute of Management\par\vspace{0.1cm}}
	{\large In study of bachelor of science, Computer Engineering
	}

	%\vfill

	% Bottom of the page
	{\large Studies year 2018 \par}
\end{titlepage}

\clearpage % End title page
\pagestyle{empty}  % No headers or footers for the following pages

\section*{Abstract}
\tab\hspace{0.3cm}The modern Reinforcement Learning has become widely interested recently, after the AlphaGo\cite{AlphaGo} became the first program to defeat a world champion in the game of Go. And by 2017, the AlphaGo Zero\cite{AlphaGoZero} and AlphaZero\cite{AlphaZero} programs achieved superhuman performance in the game of Go and Chess, by solely trained from games of self-play which require no human knowledge. But in contrast, both AlphaGo Zero and AlphaZero required extremely powerful processor as they need to random move in the early state of training which cost more expend. While in Computer Vision domain, we often use pretrained model from large dataset such as Imagenet\cite{Imagenet} and retrain it for desired task which cost less time and achieved more accuracy than train it from scratch. In this thesis, we experiment a method to reuse the trained model of a game such as Othello, Connect4 or Gomoku and re-training with one different game by hoping it to be faster.
\clearpage

\section{Introduction}
\subsection{Background}
{
\hspace{0.6cm} The modern Reinforcement Learning such as AlphaGo has showed outstanding performance by won against world champion in game of Go a decade earlier than predicted\cite{GovsCom}\cite{AGWeb}. And the AlphaGo Zero has showed that it possible to train Reinforcement Learning without any human knowledge while still achieved good performance\cite{AlphaGoZero}\cite{AlphaZero}. In contrast of good result, for AlphaGo, by using ‘supervised learning from human expert games’\cite{AlphaGo} means it's require the expert games that might be difficult to obtained or inthe worst case, it might be impossible to get. And in case of AlphaGo Zero, according to original paper\cite{AlphaGoZero}, the model need to be trained with 29 million games of self-play. Parameters were updated from 3.1 million mini-batches of 2,048 positions each. In Gian-Carlo Pascutto's experiments, every 2000 self-play moves generated by GTX-1080ti GPU would take 93 seconds, which means it need 1700 years to play all 29 million games\cite{GCP}. It practically impossible to train AlphaGo Zero in personal level.\par
\hspace{0cm} Meanwhile in Computer Vision domain, transferring weights which pretrained in large datasets and then re-train the model in preferred dataset is a widely used method as there are ImageNet\cite{Imagenet} pretrained weights in many popular libraries\cite{TFpaper}\cite{Pytorch}\cite{Keras}. The reason is it often improve regularizing and better performance and need less sample to train\cite{Transferable}. In the same way, Transfer Learning should be able to used in Deep Reinforcement Learning algorithm which use Deep Convolutional Neural Networks and help improve training speed and model's performance.\par
}
\subsection{Objective}
{
\hspace{0.6cm} 1. Create Reinforcement Learning models for board games such as Othello, Connect4 or Gomoku with AlphaZero algorithm.\par
\hspace{0cm} 2. Create Transferred Reinforcement Learning models from trained games.\par
\hspace{0cm} 3. Evaluate Transferred models performance against normal models.\par
}
\subsection{Scope}
{
\hspace{0.6cm} 1. Create at least 2 AlphaZero models and at least 2 Transferred models.\par
\hspace{0cm} 2. Evaluate performance of each algorithm in same iteration and same version.
}
\clearpage

\section{Related Works}
\subsection{AlphaGo, Mastering the game of Go with deep neural networks and tree search}
{
\hspace{0.6cm} By March 2016 DeepMind AlphaGo has become the first computer program to won against the world champion in game of Go by defeated Lee Sedol, the winner of 18 world titles, which many years earlier than predicted\cite{GovsCom}\cite{AGWeb}. The AlphaGo introduced a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state- of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. DeepMind also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away\cite{AlphaGo}.\par
\hspace{0cm} All games of perfect information have an optimal value function, \(v^*(s)\), which determines the outcome of the game, from every board position or state \(s\), under perfect play by all players. These games may be solved by recursively computing the optimal value function in a search tree containing approximately \(b^d\) possible sequences of moves, where b is the game’s breadth (number of legal moves per position) and \(d\) is its depth (game length). In large games, such as chess \((b\) ≈ \(35\), \(d\) ≈ \(80)\)\cite{AISearching} and especially Go \((b\) ≈ \(250\), \(d\) ≈ \(150)\)\cite{AISearching}, exhaustive search is infeasible\cite{Solved}\cite{Thegamesplay}, but the effective search space can be reduced by two general principles. First, the depth of the search may be reduced by position evaluation: truncating the search tree at state \(s\) and replacing the subtree below \(s\) by an approximate value function \(v(s)\) ≈ \(v^*(s)\) that predicts the outcome from state \(s\). This approach has led to superhuman performance in chess\cite{DeepBlue}, checkers\cite{Caliber} and othello\cite{BuroOth}, but it was believed to be intractable in Go due to the complexity of the game\cite{MullerComGo}. Second, the breadth of the search may be reduced by sampling actions from a policy \(p(a|s)\) that is a probability distribution over possible moves a in position \(s\). For example, Monte Carlo rollouts\cite{MCR} search to maximum depth without branching at all, by sampling long sequences of actions for both players from a policy p. Averaging over such rollouts can provide an effective position evaluation, achieving superhuman performance in backgammon\cite{MCR} and Scrabble\cite{Scrabble}, and weak amateur level play in Go\cite{Bouzy_2004}.\par
\hspace{0cm} Monte Carlo tree search (MCTS)\cite{CoulomMCTS}\cite{KocsisMCTS} uses Monte Carlo rollouts to estimate the value of each state in a search tree. As more simulations are executed, the search tree grows larger and the relevant values become more accurate. The policy used to select actions during search is also improved over time, by selecting children with higher values. Asymptotically, this policy converges to optimal play, and the evaluations converge to the optimal value function\cite{KocsisMCTS}. \par
AlphaGo pass in the board position as a 19 × 19 image and use convolutional layers to construct a representation of the position. By sing these neural networks to reduce the effective depth and breadth of the search tree: evaluating positions using a value network, and sampling actions using a policy network. The neural networks trained using a pipeline consisting of several stages of machine learning. Begin by training a supervised learning (SL) policy network \(p_\sigma\) directly from expert human moves. This provides fast, efficient learning updates with immediate feedback and high-quality gradients. And also train a fast policy \(p_\pi\) that can rapidly sample actions during rollouts. Next, train a reinforcement learning (RL) policy network \(p_\rho\) that improves the SL policy network by optimizing the final outcome of games of self-play. This adjusts the policy towards the correct goal of winning games, rather than maximizing predictive accuracy. Finally, train a value network \(v_\theta\) that predicts the winner of games played by the RL policy network against itself.\par
}
\subsection{AlphaGo Zero, Mastering the Game of Go without Human Knowledge}

\hspace{0.6cm} A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. AlphaGo Zero introduced an algorithm based solely on reinforcement learning, without human data, guidance, or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo.\par
%\hspace{0cm} Much progress towards artificial intelligence has been made using supervised learning systems that are trained to replicate the decisions of human experts. However, expert data is often expensive, unreliable, or simply unavailable. Even when reliable data is available it may impose a ceiling on the performance of systems trained in this manner\cite{HayesExpert}.In contrast, reinforcement learning systems are trained from their own experience, in principle allowing them to exceed human capabilities, and to operate in domains where human expertise is lacking. Recently, there has been rapid progress towards this goal, using deep neural networks trained by reinforcement learning. These systems have outperformed humans in computer games such as Atari\cite{Humanlevel}\cite{Atari} and 3D virtual environments\cite{Asynchronous}.\par
\hspace{0cm}AlphaGo Zero differs from first version AlphaGo in several important aspects. First and foremost, it is trained solely by self-play reinforcement learning, starting from random play, without any supervision or use of human data. Second, it only uses the black and white stones from the board as input features instead of many extracted features. Third, it uses a single neural network, rather than separate policy and value networks. Finally, it uses a simpler tree search that relies upon this single neural network to evaluate positions and sample moves, without performing any Monte-Carlo rollouts. To achieve these results, AlphaGo Zero introduced a new reinforcement learning algorithm that incorporates lookahead search inside the training loop, resulting in rapid improvement and precise and stable learning\cite{AlphaGoZero}.\par
AlphaGo Zero uses a deep neural network \(f_\theta\) with parameters \(\theta\). This neural network takes as an input the raw board representation \(s\) of the position and its history, and outputs both move probabilities and a value, \((p, v) = f_\theta(s)\). The vector of move probabilities \(p\) represents the probability of selecting each move (including pass), \(p_a = P_r(a|s)\). The value \(v\) is a scalar evaluation, estimating the probability of the current player winning from position \(s\). This neural network combines the roles of both policy network and value network\cite{AlphaGo} into a single architecture. The neural network consists of many residual blocks\cite{ResNet} of convolutional layers\cite{CNNpaper} with batch normalisation\cite{BN} and rectifier non-linearities\cite{ReLU} (see Methods).\par
\clearpage
\section{Methods}
\subsection{Reinforcement Learning}
\hspace{0.6cm}Our methods based on AlphaGo Zero's Reinforcement Learning. The neural network is trained from games of self-play by a novel reinforcement learning algorithm. In each position \(s\), an MCTS search is executed, guided by the neural network \(f_\theta\). The MCTS search outputs probabilities \(\pi\) of playing each move. These search probabilities usually select much stronger moves than the raw move probabilities \(p\) of the neural network \(f_\theta(s)\); MCTS may therefore be viewed as a powerful \textit{policy improvement} operator\cite{Howard}. Self-play with search using the improved MCTS-based policy to select each move, then using the game winner \(z\) as a sample of the value may be viewed as a powerful \textit{policy evaluation} operator. The main idea of AlphaGo Zero reinforcement learning algorithm is to use these search operators repeatedly in a policy iteration procedure: the neural network’s parameters are updated to make the move probabilities and value \((p, v) = f_\theta(s)\) more closely match the improved search probabilities and self-play winner \((\pi, z)\); these new parameters are used in the next iteration of self-play to make the search even stronger. The Monte-Carlo tree search uses the neural network \(f_\theta\) to guide its simulations. Each edge \((s, a)\) in the search tree stores a prior probability \(P(s, a)\), a visit count \(N(s, a)\), and an action-value \(Q(s, a)\). Each simulation starts from the root state and iteratively selects moves that maximise an upper confidence bound \(Q(s, a) + U(s, a)\), where \(U(s, a) \propto P(s, a)/(1 + N(s, a))\)\cite{AlphaGo}, until a leaf node \(s'\) is encountered. This leaf position is expanded and evaluated just once by the network to generate both prior probabilities and evaluation, \((P(s', ·), V(s')) = f_θ (s')\). Each edge \((s, a)\) traversed in the simulation is updated to increment its visit count \(N(s, a)\), and to \(P\) update its action-value to the mean evaluation over these simulations, \(Q(s, a) = 1/N(s, a) \sum_{s'|s,a\to s'}V(s')\), where \(s,a\to s'\) indicates that a simulation eventually reached \(s'\) after taking move a from position \(s\).\par
MCTS may be viewed as a self-play algorithm that, given neural network parameters \(\theta\) and a root position \(s\), computes a vector of search probabilities recommending moves to play, \(\pi = \alpha_\theta(s)\), proportion to the exponentiated visit count for each move, \(\pi_\alpha \propto N(s, a)^{1/\tau}\) , where \(\tau\) is a temperature parameter.\par
The neural network is trained by a self-play reinforcement learning algorithm that uses MCTS to play each move. First, the neural network is initialised to random weights \(\theta_0\). At each subsequent iteration \(i \geq 1\), games of self-play are generated. At each time-step \(t\), an MCTS search \(\pi_t = \alpha_{\theta_{i−1}}(s_t)\) is executed using the previous iteration of neural network \(f_{\theta_{i−1}}\), and a move is played by sampling the search probabilities \(\pi_t\). A game terminates at step \(T\) when both players pass, when the search value drops below a resignation threshold, or when the game exceeds a maximum length; the game is then scored to give a final reward of \(r_T \in \{−1, +1\}\). The data for each time-step \(t\) is stored as \((s_t, \pi_t, z_t)\) where \(z_t = \pm r_T\) is the game winner from the perspective of the current player at step \(t\). In parallel, new network parameters \(\theta_i\) are trained from data \((s, \pi, z)\) sampled uniformly among all time-steps of the last iteration(s) of self-play. The neural network \((p, v) = f_{\theta_i}(s)\) is adjusted to minimise the error between the predicted value \(v\) and the self-play winner \(z\), and to maximise the similarity of the neural network move probabilities \(p\) to the search probabilities \(\pi\) . Specifically, the parameters \(\theta\) are adjusted by gradient descent on a loss function \(l\) that sums over mean-squared error and cross-entropy losses respectively,\par
\begin{flushleft}\hspace{4.5cm} \((p, v) = f_θ(s),\)\hspace{1cm}\(l = (z − v)^2 − \pi^\top \log p\)\hfill(1) \end{flushleft} \par
We applied our reinforcement learning pipeline to train our programs. The base neural networks model started training from completely random behaviour and continued without human intervention over 50 iterations for 512 games each, in total 25,600 games, using 25 simulations for each MCTS. Parameters were updated using Adaptive Moment Estimation\cite{Adam} from recent 20 iterations play history with 4,096 mini-batch size(See Training Pipeline for more detail). The neural network contained 256 × 4 residual blocks(See Figure 3.). This is very few number compare to 4.9 millions games and 1,600 MCTS simulations in original AlphaGo and AlphaGo Zero but it still took more than 10 days on a single K80 card.\par

\subsection{Training Pipeline}
%\hspace{0.6cm}Our approach mostly based on AlphaGo Zero method. We used 8 × 8 board size for both Othello and Connect4 games. For better converge, we did not use weight regularisation which prevent overfitting and we use Adaptive Moment Estimation optimizer\cite{Adam} instead of stochastic gradient descent with momentum like AlphaGo and AlphaGo Zero. The neural network started training from completely random behaviour for 512 games in each iteration in total 50 iterations. We use AlphaGo Zero training approach, by updated parameters once after each iteration, rather than update every game like AlphaZero\cite{AlphaGoZero}\cite{AlphaZero}. Each move generated by 25 MCTS simulations, we did not apply Dirichlet noise.
\hspace{0.6cm}\textit{\textbf{Optimisation}} Each neural network \(f_{\theta_i}\) is optimised on a single Tesla K80 GPU using Keras with TensorFlow backend. The batch-size is 4096. Each mini-batch of data is sampled uniformly at random from all positions from the most recent 10240 games(20 iterations) of self-play. Neural network parameters are optimised by Adaptive Moment Estimation\cite{Adam} instead of stochastic gradient descent with momentum and learning rate annealing like AlphaGo and AlphaGo Zero, using the loss in Equation 1. The learning rate is fixed at 0.001, other optizer setup all according to \cite{Adam}. The cross-entropy and mean-squared error losses are weighted equally (this is reasonable because rewards are unit scaled, \(r \in \{−1, +1\}\))\par
\textit{\textbf{Evaluator}} To ensure we always generate the best quality data, we evaluate each new neural network checkpoint against the current best network \(f_{\theta_∗}\) before using it for data generation. The neural network \(f_{\theta_i}\) is evaluated by the performance of an MCTS search \(\alpha_{\theta_i}\) that uses \(f_{\theta_i}\) to evaluate leaf positions and prior probabilities (see Search Algorithm). Each evaluation consists of 100 games, using an MCTS with 25 simulations to select each move, using an infinitesimal temperature \(\tau \to 0\) (i.e. we deterministically select the move with maximum visit count, to give the strongest possible play). If the new player wins by a margin of \(> 55\%\) (to avoid selecting on noise alone) then it becomes the best player \(\alpha_{\theta_∗}\), and is subsequently used for self-play generation, and also becomes the baseline for subsequent comparisons.\par
\textit{\textbf{Self-Play}} The best current player \(\alpha_{\theta_∗}\), as selected by the evaluator, is used to generate data. In each iteration, \(\alpha_{\theta_∗}\) plays 512 games of self-play, using 25 simulations of MCTS to select each move. For the first 30 moves of each game, the temperature is set to \(\tau = 1\); this selects moves proportionally to their visit count in MCTS, and ensures a diverse set of positions are encountered. For the remainder of the game, an infinitesimal temperature is used, \(\tau \to 0\).\par
\subsection{Search Algorithm}
\hspace{0.6cm}The Search Algorithm is almost identical to AlphaGo Zero\cite{AlphaGoZero}; we recapitulate
here for completeness.\par 
Each node \(s\) in the search tree contains edges \((s, a)\) for all legal actions \(a \in A(s)\). Each edge stores a set of statistics, \[\{N(s, a), W(s, a), Q(s, a), P(s, a)\},\] where \(N(s, a)\) is the visit count, \(W(s, a)\) is the total action-value, \(Q(s, a)\) is the mean action-value, and \(P(s, a)\) is the prior probability of selecting that edge. \par
\hspace{0.6cm}\textit{\textbf{Select}} The selection phase is almost identical to AlphaGo Zero. The first in-tree phase of each simulation begins at the root node of the search tree, \(s_0\) , and finishes when the simulation reaches a leaf node \(s_L\) at time-step \(L\). At each of these time-steps, \(t < L\), an action is selected according to the statistics in the search tree, \(a_t =\) argmax \(Q(s_t, a) + U (s_t , a)\) , using a variant of the PUCT algorithm, 
\begin{center}\(U(s,a) = c_{puct}P(s,a)\frac{\sqrt{\sum_b N(s,b)}}{1+N(s,a)} \)\end{center}\par
where \(c_puct\) is a constant determining the level of exploration; this search control strategy initially prefers actions with high prior probability and low visit count, but asympotically prefers actions with high action-value.\par
\textit{\textbf{Expand and evaluate}} Positions in the queue are evaluated by the neural network using a mini-batch size of 1; the search thread is locked until evaluation completes. The leaf node is expanded and each edge (s L , a) is initialised to \({N(s_L, a) = 0, W(s_L , a) = 0, Q(s_L , a) = 0, P (s_L , a) = p_a }\); the value \(v\) is then backed up.\par
\textit{\textbf{Backup}} The edge statistics are updated in a backward pass through each step \(t \leq L\). The visit counts are incremented, \(N(s_t , a_t ) = N(s_t , a_t ) + 1)\), and the action-value is updated to the mean value, \(W(s_t , a_t ) = W(s_t , a_t) + v, Q(s_t , a_t) =\frac{W(s_t ,a_t)}{N(s_t ,a_t)}\).\par
\textit{\textbf{Play}} At the end of the search AlphaGo Zero selects a move \(a\) to play in the root position \(s_0\), proportional to its exponentiated visit count, \(\pi(a|s_0 ) = N(s_0,a)^{1/\tau} /\sum_b N (s_0 , b)^{1/\tau}\), where \(\tau\) is a temperature parameter that controls the level of exploration. The search tree is reused at subsequent time-steps: the child node corresponding to the played action becomes the new root node; the subtree below this child is retained along with all its statistics, while the remainder of the tree is discarded.
\subsection{Neural Network Architecture}
\hspace{0.6cm}The input to the neural network is a 8 × 8 × 2 image stack comprising 2 binary feature planes. The first plane \(X\) consist of binary values indicating the current player's stones position.The last plane \(Y\) represent the corresponding features for the opponent’s stones. These planes are concatenated together to give input features \(s_t = [X, Y]\).\par
The input features \(s_t\) are processed by a residual tower that consists of a single convolutional block followed by 4 residual blocks\cite{ResNet}.\par
The convolutional block applies the following modules:\par
1. A convolution of 256 filters of kernel size 3 × 3 with stride 1\par
2. Batch normalisation\cite{BN}\par
3. A rectifier non-linearity\par
Each residual block applies the following modules sequentially to its input:\par
1. A convolution of 256 filters of kernel size 3 × 3 with stride 1\par
2. Batch normalisation\par
3. A rectifier non-linearity\par
4. A convolution of 256 filters of kernel size 3 × 3 with stride 1\par
5. Batch normalisation\par
6. A skip connection that adds the input to the block\par
7. A rectifier non-linearity\par
The output of the residual tower is passed into two separate “heads” for computing the policy and value respectively. The policy head applies the following modules:\par
1. A convolution of 2 filters of kernel size 1 × 1 with stride 1\par
2. Batch normalisation\par
3. A rectifier non-linearity\par
4. A fully connected linear layer that outputs a vector of size \(8^2 = 64\) corresponding to logit probabilities for all intersections.\par
The value head applies the following modules:\par
1. A convolution of 1 filter of kernel size 1 × 1 with stride 1\par
2. Batch normalisation\par
3. A rectifier non-linearity\par
4. A fully connected linear layer to a hidden layer of size 256\par
5. A rectifier non-linearity\par
6. A fully connected linear layer to a scalar\par
7. A tanh non-linearity outputting a scalar in the range [−1, 1]\par
The overall network depth is 8 parameterised layers for the residual tower, plus an additional 2 layers for the policy head and 3 layers for the value head.
\subsection{Transfer Learning}
\hspace{0.6cm} We subsequently applied Transfer Learning to the Connect4 games trained neural network, by re-initialize parameters of the probabilities \(\theta_\rho\) and value \(\theta_\upsilon\) output layers while using the same feature extractor layers, the new move probabilities and value would became \(p = f_{\theta_\rho}(f_\theta(s))\) and \(v = f_{\theta_\upsilon}(f_\theta(s))\) respectively. \par
The transferred neural network is trained for Othello games by the same procedure as the base network for another 25600 games. We performed 2 difference networks setup for transfered training, one trained the whole network just like the base model, another one is by fixed parameters of feature extractor layers and update only new initialized output layers.\par
We evaluated the transferred models using an internal tournament against fully randomize initializing model. The internal tournament consisting 2 part, for 3 and 25 MCTS simulations respectively. As the matter of time limit, we can only compare Othello game, trained from scratch and transferred from Connect4 game one each.
\subsection{Shared Learning}
\hspace{0.6cm} We also experimented Shared Learning. By using same feature extractor layers \(f_{\theta_\gamma})\) but different input \(\theta_\iota\) and output\((\theta_\rho, \theta_\upsilon)\)(See Figure 4). Trained both games simultaneously to experiment that, is it possible to make a Neural Network that can do multiple task.\par
\textit{\textbf{Training}} We aplied shering method for Othello and Connect4 games. Each games play 512 games per iteration. The parameters \(\theta\) are updated using most recent 10240 games sample from one game(take it as Othello) and then updated using another game(take it as Connect4). The order of update is randomed to prevent bias. Another procedures are identical to prior experiment. We evaluate each new neural network checkpoint against the current best network \(f_{\theta_∗}\) before using it for data generation. The neural network \(f_{\theta_i}\) is evaluated by the performance of an MCTS search \(\alpha_{\theta_i}\) that uses \(f_{\theta_i}\) to evaluate leaf positions and prior probabilities. Each evaluation consists of 50 games for each games, intotal of 100, using an MCTS with 25 simulations to select each move. If the new player wins by a margin of \(> 55\%\) for all result, and \(> 48\%\) for each game(to avoid the network become good at on game but bad at another), then it becomes the best player \(\alpha_{\theta_∗}\), and is subsequently used for self-play generation, and also becomes the baseline for subsequent comparisons.\par
\textit{\textbf{Evaluating}} We evaluated the model using an internal tournament against fully randomize initializing model from prior experiment. The internal tournament take match for every version of each games. As the matter of time, we can only evaluated the Othello models.

\section{Result}
\subsection{Othello}
\hspace{0.6cm}The Transfer Learning experimental result is very disapointed, as the Transferred model is totally outperformed by the Fully Random Initialize(Scratch) model. The evaluating result shown in Table 1.
{
\begin{center}
\begin{tabular}{ |c|c|c|c| }
\hline
MCTS sims & Scratch & Transferred \\
\hline
5         & 85      & 15           \\
\hline
25        & 89      & 11           \\
\hline
\end{tabular}\par
\begin{small}
Table 1: \textbf{Match result}. Winning score of the internal tournament for scratch and transferred models.
\end{small}
\end{center}
}
\subsection{Shared Learning}
\hspace{0.6cm}The Shared Learning experimental result in unexpected way. The early versions of Shared model are outperformed Normal model in every version but the latter versions are worse. Figure 5 show win percentage of Shared model against normal model.
\clearpage
\begin{center}
\includegraphics[width=0.75\textwidth]{Fig5.png}\par
\begin{small}
Figure 5: Shared model win percentage, . Blue color means Shared model is better, red means worse.
\end{small}
\end{center}
\clearpage
\section{conclusions and recommendations}
\clearpage
\bibliographystyle{ieeetr}
\bibliography{testBibXeTex}

\end{document}
%\par
%\vspace{1cm}Input tensor \(\theta_{b, c, h, w}\) \par
%\vspace{1cm}Reshape to \(\theta_{b, c, h\times w}\) \par
%\vspace{1cm}Initial weights \(w_{o, c}\) \par
%\vspace{1cm}\(\theta'_b = w\times \theta_b\) \par
%\vspace{1cm}Output tensor\(\theta'_{b,o,h\times w}\) \par
%\vspace{1cm}Reshape to \(\theta'_{b,o,h,w}\)