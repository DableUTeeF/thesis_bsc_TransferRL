\documentclass[12pt,a4paper]{article}
\usepackage{makeidx}
\makeindex

\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{tabto}
\usepackage{xltxtra}
\usepackage{polyglossia}
\usepackage[top=25mm, bottom=20mm, left=25mm, right=20mm]{geometry}
\XeTeXlinebreaklocale "th"
\XeTeXlinebreakskip = 0pt plus 1pt
\defaultfontfeatures{Scale=1.23}
\title{Transferable Reinforcement Learning for Board Games}
\author{Natthaphon Hongcharoen}
%% ใช้ร่วมกับ polyglossia เพื่อให้ได้วันที่ภาษาไทย
\setdefaultlanguage{thai}
\newfontfamily{\thaifont}[Script=Thai]{TH SarabunPSK}

\begin{document}
\fontspec[
ItalicFont={TH SarabunPSK Italic},
BoldFont={TH SarabunPSK Bold},
BoldItalicFont={TH SarabunPSK Bold Italic},
]{TH SarabunPSK}

\begin{titlepage}
	\centering
	\includegraphics[width=0.15\textwidth]{logo.png}\par\vspace{1cm}
	{\scshape\Huge Transferable Reinforcement Learning for \par Board Games\par}
	\vfill
	{\Large\itshape Natthaphon Hongcharoen\par}
	\vfill
	{\Large  \par}
	{\large Thesis presented for Faculty of Engineering and Technology \par\vspace{0.1cm}}
	{\large Panyapiwat Insitute of Management\par\vspace{0.1cm}}
	{\large In study of bachelor of science, Computer Engineering
	}

	%\vfill

	% Bottom of the page
	{\large Study year 2018 \par}
\end{titlepage}

\clearpage % End title page
\pagestyle{empty}  % No headers or footers for the following pages

\section*{Abstract}
\tab\hspace{0.3cm}The modern Reinforcement Learning has become widely interested recently, after the AlphaGo\cite{AlphaGo} became the first program to defeat a world champion in the game of Go. And by 2017, the AlphaGo Zero\cite{AlphaGoZero} and AlphaZero\cite{AlphaZero} programs achieved superhuman performance in the game of Go and Chess, by solely trained from games of self-play which require no human knowledge. But in contrast, both AlphaGo Zero and AlphaZero required extremely powerful processor as they need to random move in the early state of training which cost more expend. While in Computer Vision domain, we often use pretrained model from large dataset such as Imagenet\cite{Imagenet} and retrain it for desired task which cost less time and achieved more accuracy than train it from scratch. In this thesis, we experiment a method to reuse the trained model of a game such as Othello, Connect4 or Gomoku and re-training with one different game by hoping it to be faster.
\clearpage

\section{Introduction}
\subsection{Background}
{
\hspace{0.6cm} The modern Reinforcement Learning such as AlphaGo has showed outstanding performance by won against world champion in game of Go a decade earlier than predicted\cite{GovsCom}\cite{AGWeb}. And the AlphaGo Zero has showed that it possible to train Reinforcement Learning without any human knowledge while still achieved good performance\cite{AlphaGoZero}\cite{AlphaZero}. In contrast of good result, for AlphaGo, by using ‘supervised learning from human expert games’\cite{AlphaGo} means it's require the expert games that might be difficult to obtained or inthe worst case, it might be impossible to get. And in case of AlphaGo Zero, according to original paper\cite{AlphaGoZero}, the model need to be trained with 29 million games of self-play. Parameters were updated from 3.1 million mini-batches of 2,048 positions each. In Gian-Carlo Pascutto's experiments, every 2000 self-play moves generated by GTX-1080ti GPU would take 93 seconds, which means it need 1700 years to play all 29 million games\cite{GCP}. It practically impossible to train AlphaGo Zero in personal level.\par
\hspace{0cm} Meanwhile in Computer Vision domain, transferring weights which pretrained in large datasets and then re-train the model in preferred dataset is a widely used method as there are ImageNet\cite{Imagenet} pretrained weights in many popular libraries\cite{TFpaper}\cite{Pytorch}\cite{Keras}. The reason is it often improve regularizing and better performance and need less sample to train\cite{Transferable}. In the same way, Transfer Learning should be able to used in Deep Reinforcement Learning algorithm which use Deep Convolutional Neural Networks and help improve training speed and model's performance.\par
}
\subsection{Objective}
{
\hspace{0.6cm} 1. Create Reinforcement Learning models for board games such as Othello, Connect4 or Gomoku with AlphaZero algorithm.\par
\hspace{0cm} 2. Create Transferred Reinforcement Learning models from trained games.\par
\hspace{0cm} 3. Evaluate Transferred models performance against normal models.\par
}
\subsection{Scope}
{
\hspace{0.6cm} 1. Create at least 2 AlphaZero models and at least 2 Transferred models.\par
\hspace{0cm} 2. Evaluate performance of each algorithm in same iteration and same version.
}
\clearpage

\section{Related Works}
\subsection{AlphaGo and modern Reinforcement Learning}
{
\hspace{0.6cm} By March 2016 DeepMind AlphaGo has become the first computer program to won against the world champion in game of Go by defeated Lee Sedol, the winner of 18 world titles, which many years earlier than predicted\cite{GovsCom}\cite{AGWeb}. The AlphaGo introduced a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state- of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. DeepMind also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away\cite{AlphaGo}.\par
\hspace{0cm} All games of perfect information have an optimal value function, \(v^*(s)\), which determines the outcome of the game, from every board position or state \(s\), under perfect play by all players. These games may be solved by recursively computing the optimal value function in a search tree containing approximately \(b^d\) possible sequences of moves, where b is the game’s breadth (number of legal moves per position) and \(d\) is its depth (game length). In large games, such as chess \((b\) ≈ \(35\), \(d\) ≈ \(80)\)\cite{AISearching} and especially Go \((b\) ≈ \(250\), \(d\) ≈ \(150)\)\cite{AISearching}, exhaustive search is infeasible\cite{Solved}\cite{Thegamesplay}, but the effective search space can be reduced by two general principles. First, the depth of the search may be reduced by position evaluation: truncating the search tree at state \(s\) and replacing the subtree below \(s\) by an approximate value function \(v(s)\) ≈ \(v^*(s)\) that predicts the outcome from state \(s\). This approach has led to superhuman performance in chess\cite{DeepBlue}, checkers\cite{Caliber} and othello\cite{BuroOth}, but it was believed to be intractable in Go due to the complexity of the game\cite{MullerComGo}. Second, the breadth of the search may be reduced by sampling actions from a policy \(p(a|s)\) that is a probability distribution over possible moves a in position \(s\). For example, Monte Carlo rollouts\cite{MCR} search to maximum depth without branching at all, by sampling long sequences of actions for both players from a policy p. Averaging over such rollouts can provide an effective position evaluation, achieving superhuman performance in backgammon\cite{MCR} and Scrabble\cite{Scrabble}, and weak amateur level play in Go\cite{Bouzy_2004}.\par
\hspace{0cm} Monte Carlo tree search (MCTS)\cite{CoulomMCTS}\cite{KocsisMCTS} uses Monte Carlo rollouts to estimate the value of each state in a search tree. As more simulations are executed, the search tree grows larger and the relevant values become more accurate. The policy used to select actions during search is also improved over time, by selecting children with higher values. Asymptotically, this policy converges to optimal play, and the evaluations converge to the optimal value function\cite{KocsisMCTS}. \par
AlphaGo pass in the board position as a 19 × 19 image and use convolutional layers to construct a representation of the position. By sing these neural networks to reduce the effective depth and breadth of the search tree: evaluating positions using a value network, and sampling actions using a policy network. The neural networks trained using a pipeline consisting of several stages of machine learning. Begin by training a supervised learning (SL) policy network \(p_\sigma\) directly from expert human moves. This provides fast, efficient learning updates with immediate feedback and high-quality gradients. And also train a fast policy \(p_\pi\) that can rapidly sample actions during rollouts. Next, train a reinforcement learning (RL) policy network \(p_\rho\) that improves the SL policy network by optimizing the final outcome of games of self-play. This adjusts the policy towards the correct goal of winning games, rather than maximizing predictive accuracy. Finally, train a value network \(v_\theta\) that predicts the winner of games played by the RL policy network against itself.\par
}
\subsection{AlphaGo Zero, Mastering the Game of Go without Human Knowledge}
{
%\hspace{0.6cm} A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. AlphaGo Zero introduced an algorithm based solely on reinforcement learning, without human data, guidance, or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo.\par
%\hspace{0.6cm} Much progress towards artificial intelligence has been made using supervised learning systems that are trained to replicate the decisions of human experts. However, expert data is often expensive, unreliable, or simply unavailable. Even when reliable data is available it may impose a ceiling on the performance of systems trained in this manner\cite{HayesExpert}.In contrast, reinforcement learning systems are trained from their own experience, in principle allowing them to exceed human capabilities, and to operate in domains where human expertise is lacking. Recently, there has been rapid progress towards this goal, using deep neural networks trained by reinforcement learning. These systems have outperformed humans in computer games such as Atari\cite{Humanlevel}\cite{Atari} and 3D virtual environments\cite{Asynchronous}. However, the most challenging domains in terms of human intellect such as the
\hspace{0.6cm}AlphaGo Zero differs from first version AlphaGo in several important aspects. First and foremost, it is trained solely by self-play reinforcement learning, starting from random play, without any supervision or use of human data. Second, it only uses the black and white stones from the board as input features instead of many extracted features. Third, it uses a single neural network, rather than separate policy and value networks. Finally, it uses a simpler tree search that relies upon this single neural network to evaluate positions and sample moves, without performing any Monte-Carlo rollouts. To achieve these results, we introduce a new reinforcement learning algorithm that incorporates lookahead search inside the training loop, resulting in rapid improvement and precise and stable learning\cite{AlphaGoZero}.\par
AlphaGo Zero uses a deep neural network \(f_\theta\) with parameters \(\theta\). This neural network takes as an input the raw board representation \(s\) of the position and its history, and outputs both move probabilities and a value, \((p, v) = f_\theta(s)\). The vector of move probabilities \(p\) represents the probability of selecting each move (including pass), \(p_a = P_r(a|s)\). The value \(v\) is a scalar evaluation, estimating the probability of the current player winning from position \(s\). This neural network combines the roles of both policy network and value network\cite{AlphaGo} into a single architecture. The neural network consists of many residual blocks\cite{ResNet} of convolutional layers\cite{CNNpaper} with batch normalisation\cite{BN} and rectifier non-linearities\cite{ReLU}
}
\clearpage

\bibliographystyle{siam}
\bibliography{testBibXeTex}

\end{document}
